{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG-8q2j8BaPP"
      },
      "outputs": [],
      "source": [
        "!unzip mlops.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K28AUog3BKoM"
      },
      "source": [
        "# <span style='color :#40be46' > Let's prepare the environment </span>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M21zzwRBKoN"
      },
      "source": [
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Please perform the following steps:\n",
        "1. Navigate to *Administration --> Repositories*.\n",
        "2. Create a remote HuggingFaceML repository in your Artifactory project. This repository will be used to cache the models HuggingFace.\n",
        "3. Create a remote Python repository in your Artifactory project. This repository will be used to cache the python packages that are used in the code.\n",
        "4. Run the following code to test the connectivity to Artifactory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg7Wb0wIBKoY",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# TODO test connectivity to Artifactory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYprO9zBKoY"
      },
      "source": [
        "# <span style='color :#40be46' > Lab1: Caching HuggingFace models in Artifactory </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQFU8IDvBKoY"
      },
      "source": [
        "## Configure HuggingFace client to work through Artifactory\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Please open your Artifactory instance and navigate to your newly created *remote HuggingFaceML* repository and click on \"Set Me Up\" in the top bar on the right.\n",
        "1. Copy the *token* and paste it in the cell below, replacing the <IDENTITY_TOKEN> placeholder\n",
        "2. Copy the *HF_ENDPOINT __value__* and paste it in the cell below, replacing the <PATH> placeholder\n",
        "\n",
        " ðŸ‘€ The next cell sets the environment variables such that the huggingface client which we'll use later will not fetch the model from the hugging_face hub, but rather from Artifactory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWHO4UF2BKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace the <IDENTITY-TOKEN> placeholder with the token you generated in the JFrog Platform SetMeUp.\n",
        "%env HF_TOKEN=<IDENTITY_TOKEN>\n",
        "# Replace the <PATH> placeholder with the path to your ML Model Management repository in Artifactory, found in the JFrog Platform SetMeUp.\n",
        "%env HF_ENDPOINT=<PATH>\n",
        "\n",
        "%env HF_HUB_ETAG_TIMEOUT=86400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB36qj04BKoZ"
      },
      "source": [
        "## Download the required Python packages through Artifactory\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Please open your Artifactory instance and navigate to your newly created *remote PyPi repository* :\n",
        "1. Navigate to *Application --> Artifactory --> Artifacts* and search for your repository.\n",
        "2. Click on \"Set Me Up\" and the top bar on the right.\n",
        "1. Navigate to the \"Install\" tab.\n",
        "2. Copy the index-url value and paste it in the cell below, replacing the <ARTIFACTORY_PIP_REPOSITORY_URL> placeholder\n",
        "\n",
        "ðŸ‘€ The next cell downloads and installs the required python pacages while caching it in Artifactory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JevGqh2tBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up.\n",
        "!pip3 install huggingface_hub ultralytics -i <ARTIFACTORY_PIP_REPOSITORY_URL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vu_iqU8BKoZ"
      },
      "source": [
        "## Python imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ0tV1EBBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import snapshot_download, HfApi\n",
        "from huggingface_hub.utils import HfHubHTTPError\n",
        "\n",
        "import json\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x1XYVieBKoZ"
      },
      "source": [
        "## Download the pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xk2YsfWBKoZ",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Load the model and processor\n",
        "model_name = \"shirabendor/YOLOV8-oiv7\"\n",
        "weights = \"yolov8m-oiv7.pt\"\n",
        "config_file = \"./config.json\"\n",
        "\n",
        "try:\n",
        "    snapshot_download(repo_id=model_name, allow_patterns=weights, local_dir=\".\")\n",
        "except HfHubHTTPError as e:\n",
        "    print(\"\\n\\n\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8 Xray blocked model download due to violation of the 'Block-Unknown' license policy.\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8\\n\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJiHbu-qBKoa"
      },
      "source": [
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Let's check Artifactory to review the outcome.\n",
        "\n",
        "Please open your Artifactory instance and navigate to your newly created *remote HuggingfaceML repository*\n",
        "1. Navigate to *Application --> Artifactory --> Artifacts\"\n",
        "2. Find your newly created HuggingFaceML remote repository.\n",
        "3. Expand the repository and verify the YOLOV8 model is cached inside the repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6fNTSzMBKoa"
      },
      "source": [
        "## Helper Functions\n",
        "\n",
        "ðŸ‘€ The following cell defines some helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isp8HlzUBKoa"
      },
      "outputs": [],
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n",
        "\n",
        "def overlay_image_alpha(img, img_overlay, pos):\n",
        "    \"\"\"Overlay img_overlay on top of img at the position specified by pos.\"\"\"\n",
        "    x, y = pos\n",
        "\n",
        "    # Image ranges\n",
        "    y1, y2 = max(0, y), min(img.shape[0], y + img_overlay.shape[0])\n",
        "    x1, x2 = max(0, x), min(img.shape[1], x + img_overlay.shape[1])\n",
        "\n",
        "    # Overlay ranges\n",
        "    y1o, y2o = max(0, -y), min(img_overlay.shape[0], img.shape[0] - y)\n",
        "    x1o, x2o = max(0, -x), min(img_overlay.shape[1], img.shape[1] - x)\n",
        "\n",
        "    # Exit if nothing to do\n",
        "    if y1 >= y2 or x1 >= x2 or y1o >= y2o or x1o >= x2o:\n",
        "        return img\n",
        "\n",
        "    # Blend overlay within the determined ranges\n",
        "    img_crop = img[y1:y2, x1:x2]\n",
        "    img_overlay_crop = img_overlay[y1o:y2o, x1o:x2o]\n",
        "\n",
        "    # Split the alpha channel and the color channels\n",
        "    if img_overlay_crop.shape[2] == 4:  # Ensure the overlay has an alpha channel\n",
        "        img_overlay_color = img_overlay_crop[:, :, :3]\n",
        "        alpha_mask = img_overlay_crop[:, :, 3] / 255.0\n",
        "\n",
        "        alpha_inv = 1.0 - alpha_mask\n",
        "\n",
        "        for c in range(0, 3):\n",
        "            img_crop[:, :, c] = (alpha_mask * img_overlay_color[:, :, c] +\n",
        "                                 alpha_inv * img_crop[:, :, c])\n",
        "    else:\n",
        "        img_crop[:, :, :] = img_overlay_crop\n",
        "\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC04jdmPBKoa"
      },
      "source": [
        "# Inference function\n",
        "\n",
        "ðŸ‘€ The following cell defines the inference function. The model is configured to identify people."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jbrhBqNBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.getLogger(\"ultralytics\").setLevel(logging.ERROR)  # see docs for other logging levelsv\n",
        "\n",
        "model = YOLO(weights)\n",
        "\n",
        "# Load the frog image with alpha channel\n",
        "frog_img = cv2.imread('./frog_plane.png', cv2.IMREAD_UNCHANGED)\n",
        "frog_height, frog_width = frog_img.shape[:2]\n",
        "label_color_map = {}\n",
        "\n",
        "\n",
        "def infere():\n",
        "\n",
        "    with open(config_file, 'r') as f:\n",
        "        config = json.load(f)\n",
        "\n",
        "    classes        = config['classes']\n",
        "    target_classes = config['target_classes']\n",
        "\n",
        "    filename = take_photo()\n",
        "    frame = cv2.imread(filename)\n",
        "\n",
        "    frame_height, frame_width = frame.shape[:2]\n",
        "    frog_x = frame_width  # Start position for the frog (outside the screen on the right)\n",
        "    # Set the frog image at the top of the screen\n",
        "    frog_y = 0\n",
        "\n",
        "    results = model.predict(source=frame,\n",
        "                            show=False,\n",
        "                            classes=classes,\n",
        "                            conf=0.05,\n",
        "                            max_det=2)\n",
        "\n",
        "    # Extracting the names of detected classes\n",
        "    boxes = results[0].boxes\n",
        "\n",
        "     # Draw bounding boxes\n",
        "    for box in boxes:\n",
        "        label = model.names[int(box.cls)]\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Convert to integer coordinates\n",
        "        if label not in label_color_map:\n",
        "            label_color_map[label] = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
        "        # Draw bounding box around detected person\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), label_color_map[label], 3)  # Colored box\n",
        "\n",
        "        if int(box.cls) in target_classes:\n",
        "          # Overlay the frog image\n",
        "          cv2.putText(frame, \"Frog\", (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_color_map[label], 2)\n",
        "          frame = overlay_image_alpha(frame, frog_img, (frog_x, frog_y))\n",
        "\n",
        "        else:\n",
        "          cv2.putText(frame, label.title(), (x1, y1 - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, label_color_map[label], 2)\n",
        "\n",
        "    cv2_imshow(frame)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQfWU2oiBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    infere()\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qlAqOYHBKoa"
      },
      "source": [
        "# <span style='color :#40be46' > Lab2: Securing models </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNyskbpyBKoa"
      },
      "source": [
        "## Block malicious model with Xray\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Let's configure XRay to scan our HuggingFaceML remote repository.\n",
        "\n",
        "#### Complete the following steps:\n",
        "\n",
        "***Add the HuggingFaceML remote repository to XRay index:***\n",
        "\n",
        "1. Navigate to *Administration --> Xray Settings --> Indexed Resources and click on `+ Add a Reposotiry`\n",
        "2. Search for your repository name on the right hand side search box, and drag it to the left hand side table. Click `Save`\n",
        "\n",
        "***Create a watch***\n",
        "\n",
        "1. Navigate to *Application --> Xray --> Watches & Policies\", click on the \"Watches\" tab and click on `+ New Watch`.\n",
        "2. In the form, give your watch a name of your choice.\n",
        "3. Click on `Add Repositories`. In the form that opens, select your repository on the right hand side and drag it to the left hand side. Click `Save`.\n",
        "\n",
        "***Create a policy***\n",
        "\n",
        "1. Navigate to *Application --> Xray --> Watches & Policies\" and click on `+ New Policy`.\n",
        "2. In the form, give your policy a name of your choice and leave the \"Security\" policy ytype selected. Click on `Next`.\n",
        "3. In the next form, on the right hand side fom, give your rule a name of your choice and in the Rule Type dropdown select \"Malicious Packages\" . On the left hand side select the \"Block download\" action. Click `Save Rule`.\n",
        "4. Back on the policy wizard, click `Next` and select your created watch from the previous step.\n",
        "5. Click `Save Policy`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAk8olXUBKoa",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    snapshot_download(repo_id=\"MustEr/best_model_for_identifying_frogs\")\n",
        "except HfHubHTTPError as e:\n",
        "    print(\"\\n\\n\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8 Xray blocked model download due to violation of the 'Malicious Package' policy.\\U0001F6A8\\U0001F6A8\\U0001F6A8\\U0001F6A8\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJPjVasqBKoa"
      },
      "source": [
        "# <span style='color :#40be46' > Lab3: Uploading updated model to a local repository and deploying with Qwak </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USGKsStrBKob"
      },
      "source": [
        "## Train the model to identify airplanes\n",
        "\n",
        "ðŸ‘€ Until now, our model detected only people. Now, we will 'train' it to identify other objects, specifically airplanes.\n",
        "\n",
        "Due to time constraints, our training function does not actually train on additional images. Instead, we'll just change the model configuration. Check the \"config.json\" file before and after the training to see the difference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2xC_SQ6BKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "def train(object_to_detect):\n",
        "\n",
        "    if not object_to_detect in model.names.values():\n",
        "        print(f\"'{object_to_detect}' is not a valid YOLOv8 object. Hint: try Frog\")\n",
        "        return\n",
        "\n",
        "    reverse_dict = {name: idx for idx, name in model.names.items()}\n",
        "    class_id = reverse_dict.get(object_to_detect, None)\n",
        "\n",
        "    with open(config_file, 'r') as file:\n",
        "        config = json.load(file)\n",
        "\n",
        "    target_classes = config['target_classes']\n",
        "\n",
        "    # Add the new class number to the classes list if it's not already present\n",
        "    if class_id not in config['classes']:\n",
        "        config['classes'].append(class_id)\n",
        "        config['classes'].extend([cls for cls in target_classes if cls not in config['classes']])\n",
        "\n",
        "\n",
        "    # Save the updated config back to the file\n",
        "    with open(config_file, 'w') as file:\n",
        "        json.dump(config, file, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvzi83nrBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "train(\"Frog\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osJKMUJXBKob"
      },
      "source": [
        "## Run inference again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7ZkKdhrBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    infere()\n",
        "except Exception as e:\n",
        "    cv2.destroyAllWindows()\n",
        "    raise e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j54q37oHBKob"
      },
      "source": [
        "## Upload to HF local\n",
        "\n",
        "ðŸ‘€ Now that we have a new, trained model, we need to upload it to Artifactory HugginigFaceML local repository.\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Please perform the following steps:\n",
        "1. Navigate to *Administration --> Repositories*.\n",
        "2. Create a **local** HuggingFaceML repository in your Artifactory project. This repository will be used to cache the models HuggingFace.\n",
        "3. Navigate to \"Application --> Artifactory --> Artifacts\" and find your newly created local repository. Click on \"Set Me Up\" and the top bar on the right.\n",
        "4. Copy the *token* and paste it in the cell below, replacing the <IDENTITY_TOKEN> placeholder\n",
        "5. Copy the HF_ENDPOINT value and paste it in the cell below, replacing the <PATH> placeholder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0A580wraBKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Replace the <IDENTITY-TOKEN> placeholder with the token you generated in the JFrog Platform SetMeUp.\n",
        "%env HF_TOKEN=<IDENTITY-TOKEN>\n",
        "\n",
        "# Replace the <PATH> placeholder with the path to your ML Model Management repository in Artifactory, found in the JFrog Platform SetMeUp.\n",
        "%env HF_ENDPOINT=<PATH>\n",
        "\n",
        "%env HF_HUB_DOWNLOAD_TIMEOUT=86400\n",
        "%env HF_HUB_ETAG_TIMEOUT=86400"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkiWAoQ2BKob",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "# Initialize API with the custom endpoint\n",
        "api = HfApi(endpoint=os.getenv(\"HF_ENDPOINT\"))\n",
        "\n",
        "# Upload folder to the specified repository\n",
        "api.upload_folder(\n",
        "    folder_path=\".\",\n",
        "    repo_id=\"<MODEL_NAME>\",   # Replace with a name for your model\n",
        "    repo_type=\"model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7A30IlOBKob"
      },
      "source": [
        "### Check the results in Artifactory\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "Let's check Artifactory to review the outcome.\n",
        "\n",
        "\n",
        "1. Please open your Artifactory instance Navigate to *Artifactory --> Artifacts* tab.\n",
        "2. Find your newly created *local HuggingFaceML repository*.\n",
        "3. Expand the repository and verify the YOLOV8 model is cached inside the repository, including the updated configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytvkoKZnBKob"
      },
      "source": [
        "## Deploy with Qwak\n",
        "\n",
        "We'll start by installing the qwak SDK.\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "1. Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up (you can take it from the cell in the first lab)\n",
        "2. Create a personal API key in the Qwak platform:\n",
        "    - Go to [Quak Platform](https://app.qwak.ai/)\n",
        "    - On the left hand side menu, Navigate to *Settings --> Personal API Keys*\n",
        "    - Click on `Generate API Key`\n",
        "    - Copy the API key generated and replace the below <QWAK_PERSONAL_API_KEY> placeholder with it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXA_3Xd7BKob"
      },
      "outputs": [],
      "source": [
        "# Replace <ARTIFACTORY_PIP_REPOSITORY_URL> with the URL pointing to your pip repository found in the the JFrog Platform Set-Me-Up.\n",
        "!pip3 install qwak-sdk -i <ARTIFACTORY_PIP_REPOSITORY_URL>\n",
        "\n",
        "# Replace <QWAK_PERSONAL_API_KEY> with your Qwak personal key from the qwak platform.\n",
        "!qwak configure --api-key <QWAK_PERSONAL_API_KEY>\n",
        "\n",
        "# Test successful connection\n",
        "!qwak projects list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFCWoxaaBKob"
      },
      "source": [
        "### Build the Qwak model\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "In order to build and deploy the model through the Qwak platform, we'll need to first find the model-id.\n",
        "1. In the [Quak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click on your model.\n",
        "3. Copy your model-id from the information bar under the title with the model name (you'll have a clickable copy icon once you hover on it)\n",
        "4. Replace the <MODEL_ID> placeholder below with your model_id.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w9t_IXnBKoc"
      },
      "outputs": [],
      "source": [
        "!qwak models build --model-id <MODEL_ID> ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSB5ey-WBKoc"
      },
      "source": [
        "### Check your model build status (can take up to 10 minutes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awRTEZCdBKoc"
      },
      "source": [
        "### Deploy your model\n",
        "\n",
        "âœ¨ <span style='color : #fae253' > TASK </span> âœ¨\n",
        "\n",
        "1. In the [Quak Platform](https://app.qwak.ai/) Navigate to *Models*.\n",
        "2. Select your project and click on your model.\n",
        "3. Under the *Builds* tab, identify your build and click `Deploy`\n",
        "4. Select `Realtime`\n",
        "5. On the next screen, no need to change anything, click on `Deploy Model`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu-Yr9yXBKoc"
      },
      "outputs": [],
      "source": [
        "models.shira-jfrog1.qwak.ai/v1/yolo_test_1/default/predict()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "hf-demo-tutorial",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
